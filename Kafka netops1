from confluent_kafka import Consumer, KafkaException
from prometheus_client import start_http_server, Gauge
import time
import json

# Configure Kafka consumer
consumer_config = {
    'bootstrap.servers': 'your_kafka_server',
    'group.id': 'your_group',
    'auto.offset.reset': 'earliest',
    'security.protocol': 'SSL',
    'ssl.ca.location': 'path_to_ca.pem',
    'ssl.certificate.location': 'path_to_cert.pem',
    'ssl.key.location': 'path_to_key.pem'
}

consumer = Consumer(consumer_config)
consumer.subscribe(['your_kafka_topic'])

# Prometheus Gauge to store metrics
your_metric_gauge = Gauge('dx_netops_metric', 'Description of metric', ['label'])

# Start Prometheus HTTP server
start_http_server(8000)  # Exposes metrics on http://localhost:8000/metrics

# Poll Kafka every 5 minutes
try:
    while True:
        # Poll for new messages
        msg = consumer.poll(timeout=1.0)

        if msg is None:
            # No message received, wait for next poll
            time.sleep(1)
            continue
        elif msg.error():
            # Handle consumer errors
            if msg.error().code() == KafkaException._PARTITION_EOF:
                # End of partition event
                print(f"Reached end of partition for topic {msg.topic()}")
            else:
                print(f"Consumer error: {msg.error()}")
            continue

        # Process the message if no errors
        try:
            # Parse the Kafka message
            data = json.loads(msg.value().decode('utf-8'))
            
            # Extract relevant metrics and update Prometheus Gauge
            metric_value = data.get('metric_key')  # Adjust to your specific data structure
            your_metric_gauge.labels(label='example_label').set(metric_value)
        
        except json.JSONDecodeError:
            print("Failed to decode JSON from Kafka message")

        # Sleep for 5 minutes before polling Kafka again
        time.sleep(300)
finally:
    # Ensure the consumer is properly closed
    consumer.close()
